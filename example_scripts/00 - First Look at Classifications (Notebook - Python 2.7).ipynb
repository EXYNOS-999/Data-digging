{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I downloaded my classifications; now what?\n",
    "\n",
    "This notebook will help you take a first look at your data, to get basic information about it like: how many classifications, how many classifiers (signed in and not signed in), etc. It uses Python 2.7.\n",
    "\n",
    "There are scripts to do this, but first let's just get a sense of what the data looks like.\n",
    "\n",
    "Before we begin, though, we will need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 2.7.11, numpy version: 1.11.0, pandas version: 0.19.2. \n",
      "Originally developed using Py 2.7.11, np v1.11.0, pd v0.19.2\n",
      "If these versions don't match and stuff breaks, that's probably why.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Python version: %d.%d.%d, numpy version: %s, pandas version: %s. \\nOriginally developed using Py 2.7.11, np v1.11.0, pd v0.19.2\" %(sys.version_info[0], sys.version_info[1], sys.version_info[2], np.__version__, pd.__version__))\n",
    "print(\"If these versions don't match and stuff breaks, that's probably why.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's say your project is called \"My Project\". We'll make that a variable below, because any of the files we need to access (classifications file, workflow contents file, etc) will start with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-project-classifications.csv\n"
     ]
    }
   ],
   "source": [
    "project_name = \"my-project\"\n",
    "\n",
    "classification_file = project_name + \"-classifications.csv\"\n",
    "\n",
    "print(classification_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in that file, using a package called `pandas` which is designed to handle large tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File my-project-classifications.csv read with 50000 rows.\n"
     ]
    }
   ],
   "source": [
    "classifications_all = pd.read_csv(classification_file)\n",
    "n_class = len(classifications_all)\n",
    "\n",
    "print(\"File %s read with %d rows.\" % (classification_file, n_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows, which we've saved as `n_class`, is the same as the total number of classifications recorded in this file. \n",
    "\n",
    "  **Note:** The more classifications in your file, the more memory it will take for your computer to work with them using `pandas`. From my experience, a few million rows isn't too big a deal as long as you have at least 8 GB of RAM. If you have a lot more, you may need something with more memory than a laptop, or you might want to use a script that doesn't try to hold them all in memory at once, or a package meant to be parallelized, like `dask`.\n",
    "\n",
    "What does each classification actually contain? Here are the column headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'classification_id', u'user_name', u'user_id', u'user_ip',\n",
       "       u'workflow_id', u'workflow_version', u'created_at', u'metadata',\n",
       "       u'subject_ids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the file (i.e., each classification) includes:\n",
    "\n",
    " - **classification_id** - the unique ID assigned to each classification\n",
    " - **user_name** - the username the classifier chose when they registered on the site (this is public-facing as it's what they're identified with when they post on Talk)\n",
    " - **user_id** - the user's ID number in the Zooniverse database (this is not public; in the example file they've been hashed)\n",
    " - **user_ip** - a hashed version of the user's IP address\n",
    " - **workflow_id** - the ID number of the workflow this classification was recorded in\n",
    " - **workflow_name** - the text name of the workflow this classification was recorded in\n",
    " - **workflow_version** - the version number (format `major.minor`) of the workflow\n",
    " - **created_at** - the timestamp from when the classification was recorded\n",
    " - **metadata** - metadata from the classification such as browser information, operating system used\n",
    " - **annotations** - the actual information from the classification (answers / clicks / species identifications / etc, specific to this workflow id+version)\n",
    " - **subject_data** - the data on the subject that was uploaded as part of the subject upload\n",
    " - **subject_ids** - the unique identifier of all subjects classified in this classification (typically 1 subject)\n",
    " \n",
    "We can also quickly look at the first few rows in raw form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>workflow_id</th>\n",
       "      <th>workflow_version</th>\n",
       "      <th>created_at</th>\n",
       "      <th>metadata</th>\n",
       "      <th>subject_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71367607</td>\n",
       "      <td>SM-Ystrad</td>\n",
       "      <td>2523977.0</td>\n",
       "      <td>e6fabaa028b11310d86c</td>\n",
       "      <td>5030</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2017-09-21 14:10:26 UTC</td>\n",
       "      <td>{\"session\":\"fa647c7c26d6098c7eb8a3eaebcfa1d818...</td>\n",
       "      <td>13097531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71367608</td>\n",
       "      <td>Earley</td>\n",
       "      <td>2525815.0</td>\n",
       "      <td>5c6c5c6fcf8c4210d3cd</td>\n",
       "      <td>5030</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2017-09-21 14:10:26 UTC</td>\n",
       "      <td>{\"session\":\"c56326bf3ef998d4eac01372242c6116c5...</td>\n",
       "      <td>13096685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71367609</td>\n",
       "      <td>grahamg3</td>\n",
       "      <td>2461231.0</td>\n",
       "      <td>63320c40f9ef88eb050f</td>\n",
       "      <td>5030</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2017-09-21 14:10:26 UTC</td>\n",
       "      <td>{\"session\":\"9c6fa15b076b3dbdb14f043a4710f3f81f...</td>\n",
       "      <td>13095065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71367612</td>\n",
       "      <td>Salomien</td>\n",
       "      <td>2199368.0</td>\n",
       "      <td>4fbc7f6f4c7207640bd8</td>\n",
       "      <td>5030</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2017-09-21 14:10:27 UTC</td>\n",
       "      <td>{\"session\":\"e7ce05de5b943db614c2c7117e0f2d212f...</td>\n",
       "      <td>13097380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71367613</td>\n",
       "      <td>Amonite</td>\n",
       "      <td>2426278.0</td>\n",
       "      <td>0104c1ddea96f5f5034b</td>\n",
       "      <td>5030</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2017-09-21 14:10:27 UTC</td>\n",
       "      <td>{\"session\":\"503b9a2d612801caefdda0abe0dd5ed451...</td>\n",
       "      <td>13096378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classification_id  user_name    user_id               user_ip  workflow_id  \\\n",
       "0           71367607  SM-Ystrad  2523977.0  e6fabaa028b11310d86c         5030   \n",
       "1           71367608     Earley  2525815.0  5c6c5c6fcf8c4210d3cd         5030   \n",
       "2           71367609   grahamg3  2461231.0  63320c40f9ef88eb050f         5030   \n",
       "3           71367612   Salomien  2199368.0  4fbc7f6f4c7207640bd8         5030   \n",
       "4           71367613    Amonite  2426278.0  0104c1ddea96f5f5034b         5030   \n",
       "\n",
       "   workflow_version               created_at  \\\n",
       "0               3.8  2017-09-21 14:10:26 UTC   \n",
       "1               3.8  2017-09-21 14:10:26 UTC   \n",
       "2               3.8  2017-09-21 14:10:26 UTC   \n",
       "3               3.8  2017-09-21 14:10:27 UTC   \n",
       "4               3.8  2017-09-21 14:10:27 UTC   \n",
       "\n",
       "                                            metadata  subject_ids  \n",
       "0  {\"session\":\"fa647c7c26d6098c7eb8a3eaebcfa1d818...     13097531  \n",
       "1  {\"session\":\"c56326bf3ef998d4eac01372242c6116c5...     13096685  \n",
       "2  {\"session\":\"9c6fa15b076b3dbdb14f043a4710f3f81f...     13095065  \n",
       "3  {\"session\":\"e7ce05de5b943db614c2c7117e0f2d212f...     13097380  \n",
       "4  {\"session\":\"503b9a2d612801caefdda0abe0dd5ed451...     13096378  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if you ignore the classification annotations themselves, there's still a lot of information in this classification file. Let's find out some other basic information about the classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 classifications from 919 classifiers, of which 742 (81 percent) were signed-in and 177 (19 percent) were not signed in.\n",
      "\n",
      "Average classifications per user: 54.4\n"
     ]
    }
   ],
   "source": [
    "users_all = classifications_all['user_name'].unique()\n",
    "n_users = len(users_all)\n",
    "\n",
    "# if the classification is from a classifier who isn't signed in, the user_name field has \"not-logged-in-[user_ip]\"\n",
    "is_unreg = np.array([q.startswith(\"not-logged-in\") for q in users_all])\n",
    "is_reg   = np.invert(is_unreg)\n",
    "\n",
    "n_unreg = sum(is_unreg)\n",
    "n_reg   = sum(is_reg)\n",
    "\n",
    "print(\"%d classifications from %d classifiers, of which %d (%.0f percent) were signed-in and %d (%.0f percent) were not signed in.\\n\" % (n_class, n_users, n_reg, (float(n_reg)/float(n_users)*100.), n_unreg, (float(n_unreg)/float(n_users)*100.)))\n",
    "\n",
    "print(\"Average classifications per user: %.1f\" % (float(n_class)/float(n_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifications registered between 2017-09-21 14:10:26 UTC and 2017-09-22 16:41:17 UTC.\n"
     ]
    }
   ],
   "source": [
    "# use created_at to print date range for classifications\n",
    "print(\"Classifications registered between %s and %s.\" % (classifications_all['created_at'][classifications_all.index[0]], classifications_all['created_at'][classifications_all.index[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest classification ID in this file: 71523950\n"
     ]
    }
   ],
   "source": [
    "# print out the classification ID of the last classification (useful in some cases)\n",
    "print(\"Latest classification ID in this file: %d\" % classifications_all['classification_id'][classifications_all.index[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more we could do here: compute medians as well as averages, figure out the typical time it takes for a user to complete a classification, work out how many hours of human effort were spent classifying, etc. We could also clean the classification export of duplicate and non-live classifications, and isolate classifications from just the workflow ID + version that we want to actually analyze.\n",
    "\n",
    "However, that's for the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
